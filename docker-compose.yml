services:
  # ============================================
  # FRONTEND APPLICATION
  # ============================================
  frontend:
    build:
      context: ./frontend/ai-front
      dockerfile: Dockerfile
    container_name: ai_infra_frontend
    restart: unless-stopped
    networks:
      - frontend-net
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # REVERSE PROXY
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: ai_infra_nginx
    restart: unless-stopped
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "${NGINX_PORT:-80}:80"
    networks:
      - frontend-net
      - monitoring-net
    depends_on:
      - frontend
      - grafana
      - prometheus
      - tempo
      - loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # MONITORING: PROMETHEUS
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    container_name: ai_infra_prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.external-url=/monitoring/prometheus'
      - '--web.route-prefix=/'
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./docker/prometheus/alerts:/etc/prometheus/alerts
      - prometheus_data:/prometheus
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: GRAFANA
  # ============================================
  grafana:
    image: grafana/grafana:latest
    container_name: ai_infra_grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: ${GRAFANA_PLUGINS:-}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SERVER_ROOT_URL: "%(protocol)s://%(domain)s:%(http_port)s/monitoring/grafana/"
      GF_SERVER_SERVE_FROM_SUB_PATH: true
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - monitoring-net
    depends_on:
      - prometheus
      - tempo
      - loki
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

  # ============================================
  # MONITORING: TEMPO (Distributed Tracing)
  # ============================================
  tempo:
    image: grafana/tempo:latest
    container_name: ai_infra_tempo
    restart: unless-stopped
    command: ["-config.file=/etc/tempo/tempo.yml"]
    volumes:
      - ./docker/tempo/tempo.yml:/etc/tempo/tempo.yml
      - tempo_data:/var/tempo
    # No ports exposed - all access via nginx at /monitoring/tempo/
    # OTLP receivers (4317, 4318) available internally within monitoring-net
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: LOKI (Log Aggregation)
  # ============================================
  loki:
    image: grafana/loki:latest
    container_name: ai_infra_loki
    restart: unless-stopped
    command: ["-config.file=/etc/loki/loki.yml"]
    volumes:
      - ./docker/loki/loki.yml:/etc/loki/loki.yml
      - loki_data:/loki
    # HTTP UI accessible via nginx at /monitoring/loki/
    # Port 3100 used internally for log ingestion from Promtail/clients
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

# ============================================
# NETWORKS
# ============================================
networks:
  frontend-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/24
  monitoring-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/24

# ============================================
# VOLUMES
# ============================================
volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  tempo_data:
    driver: local
  loki_data:
    driver: local
