services:
  # ============================================
  # FRONTEND APPLICATION
  # ============================================
  # Built from external AI_Front repository (../AI_Front)
  frontend:
    build:
      context: ../AI_Front
      dockerfile: Dockerfile
      args:
        # Build-time environment variables for Vite
        # These are embedded into the JavaScript bundle at build time
        VITE_API_BASE_URL: ${VITE_API_BASE_URL:-http://localhost/api}
        VITE_GRAFANA_URL: ${VITE_GRAFANA_URL:-http://localhost/monitoring/grafana/}
        # Keycloak must use nginx reverse proxy, not direct port
        VITE_KEYCLOAK_URL: ${VITE_KEYCLOAK_URL:-http://localhost/auth}
        VITE_KEYCLOAK_REALM: ${VITE_KEYCLOAK_REALM:-infra-admin}
        VITE_KEYCLOAK_CLIENT_ID: ${VITE_KEYCLOAK_CLIENT_ID:-ai-front-spa}
        VITE_KEYCLOAK_MIN_VALIDITY: ${VITE_KEYCLOAK_MIN_VALIDITY:-70}
        VITE_KEYCLOAK_CHECK_INTERVAL: ${VITE_KEYCLOAK_CHECK_INTERVAL:-60}
        VITE_KEYCLOAK_DEBUG: ${VITE_KEYCLOAK_DEBUG:-false}
        VITE_ENV: ${VITE_ENV:-production}
    container_name: ai_infra_frontend
    restart: unless-stopped
    networks:
      - frontend-net
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # REVERSE PROXY
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: ai_infra_nginx
    restart: unless-stopped
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "${NGINX_PORT:-80}:80"
    networks:
      - frontend-net
      - monitoring-net
      - storage-net
    depends_on:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://127.0.0.1:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # MONITORING: PROMETHEUS
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    container_name: ai_infra_prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
      - '--web.external-url=/monitoring/prometheus'
      - '--web.route-prefix=/'
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./docker/prometheus/alerts:/etc/prometheus/alerts
      - prometheus_data:/prometheus
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: GRAFANA
  # ============================================
  grafana:
    image: grafana/grafana:latest
    container_name: ai_infra_grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: ${GRAFANA_PLUGINS:-}
      GF_USERS_ALLOW_SIGN_UP: false
      # Subpath configuration for reverse proxy
      # serve_from_sub_path=true: Grafana expects the full path including /monitoring/grafana
      # NGINX will pass the complete path without stripping
      GF_SERVER_ROOT_URL: http://localhost/monitoring/grafana
      GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      GF_SERVER_DOMAIN: localhost
      # Config file path
      GF_PATHS_CONFIG: /etc/grafana/grafana.ini
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
      - ./docker/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    networks:
      - monitoring-net
    depends_on:
      - prometheus
      - tempo
      - loki
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

  # ============================================
  # MONITORING: TEMPO (Distributed Tracing)
  # ============================================
  tempo:
    image: grafana/tempo:latest
    container_name: ai_infra_tempo
    restart: unless-stopped
    command: ["-config.file=/etc/tempo/tempo.yml"]
    volumes:
      - ./docker/tempo/tempo.yml:/etc/tempo/tempo.yml
      - tempo_data:/var/tempo
    # No ports exposed - all access via nginx at /monitoring/tempo/
    # OTLP receivers (4317, 4318) available internally within monitoring-net
    networks:
      - monitoring-net
    depends_on:
      - prometheus
    # Note: Tempo image is minimal and doesn't include wget/curl/sh
    # Healthcheck disabled - Tempo's availability is monitored via Prometheus metrics
    # Service is considered healthy when it starts successfully
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # TESTING: TRACE GENERATOR (Optional - for Tempo testing)
  # ============================================
  # Uncomment to generate test traces for Tempo dashboard
  tempo-trace-generator:
    build:
      context: ./docker/tempo
      dockerfile: Dockerfile.test
    container_name: ai_infra_tempo_trace_generator
    restart: unless-stopped
    networks:
      - monitoring-net
    depends_on:
      - tempo
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M

  # ============================================
  # MONITORING: LOKI (Log Aggregation)
  # ============================================
  loki:
    image: grafana/loki:latest
    container_name: ai_infra_loki
    restart: unless-stopped
    command: ["-config.file=/etc/loki/loki.yml"]
    volumes:
      - ./docker/loki/loki.yml:/etc/loki/loki.yml
      - loki_data:/loki
    # HTTP UI accessible via nginx at /monitoring/loki/
    # Port 3100 used internally for log ingestion from Promtail/clients
    networks:
      - monitoring-net
    # Note: Loki image is minimal and doesn't include wget/curl/sh
    # Healthcheck disabled - Loki's availability is monitored via Prometheus metrics
    # Service is considered healthy when it starts successfully
    healthcheck:
      disable: true
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: PROMTAIL (Log Collection Agent)
  # ============================================
  promtail:
    image: grafana/promtail:latest
    container_name: ai_infra_promtail
    restart: unless-stopped
    command: ["-config.file=/etc/promtail/promtail.yml"]
    volumes:
      - ./docker/promtail/promtail.yml:/etc/promtail/promtail.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - monitoring-net
    depends_on:
      - loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # CACHE & MESSAGE BROKER: REDIS
  # ============================================
  redis:
    image: redis:7-alpine
    container_name: ai_infra_redis
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    networks:
      - database-net
      - monitoring-net
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ============================================
  # DATABASE: POSTGRESQL
  # ============================================
  postgres:
    image: postgres:16-alpine
    container_name: ai_infra_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-app_db}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF-8"
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_LOG_LEVEL: ${POSTGRES_LOG_LEVEL:-WARNING}
      POSTGRES_SLOW_QUERY_MS: ${POSTGRES_SLOW_QUERY_MS:-1000}
      # Keycloak database credentials
      KEYCLOAK_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro
    command: postgres -c config_file=/etc/postgresql/postgresql.conf -c hba_file=/etc/postgresql/pg_hba.conf
    networks:
      - database-net
      - monitoring-net
    labels:
      com.docker.compose.service: "postgres"
      logging.source: "postgres"
      logging.type: "database"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-app_db}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # IDENTITY & ACCESS MANAGEMENT: KEYCLOAK
  # ============================================
  keycloak:
    image: quay.io/keycloak/keycloak:latest
    container_name: ai_infra_keycloak
    restart: unless-stopped
    command: start-dev --import-realm
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
      KC_DB_SCHEMA: public
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HOSTNAME_STRICT_BACKCHANNEL: false
      KC_HTTP_ENABLED: true
      KC_PROXY: edge
      KC_PROXY_ADDRESS_FORWARDING: true
      KC_HEALTH_ENABLED: true
      KC_METRICS_ENABLED: true
      KC_LOG_LEVEL: ${KEYCLOAK_LOG_LEVEL:-INFO}
    volumes:
      - keycloak_data:/opt/keycloak/data
      - ./docker/keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json:ro
      - ./docker/keycloak/keycloak.conf:/opt/keycloak/conf/keycloak.conf:ro
    networks:
      - frontend-net
      - database-net
      - monitoring-net
    labels:
      com.docker.compose.service: "keycloak"
      logging.source: "keycloak"
      logging.type: "identity_provider"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # ============================================
  # DATABASE ADMIN: PGADMIN
  # ============================================
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: ai_infra_pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: "True"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
      PGADMIN_LOG_LEVEL: ${PGADMIN_LOG_LEVEL:-INFO}
      SCRIPT_NAME: /pgadmin
      ENVIRONMENT: ${ENVIRONMENT:-development}
      # Gunicorn timeout configuration (in seconds) - prevent startup errors
      GUNICORN_TIMEOUT: ${PGADMIN_GUNICORN_TIMEOUT:-60}
      # Keycloak OIDC Configuration
      # Note: AUTHENTICATION_SOURCES is configured in config_distro.py and config_local.py
      PGADMIN_CONFIG_OAUTH2_AUTO_CREATE_USER: ${PGADMIN_OAUTH2_AUTO_CREATE_USER:-True}
      PGADMIN_OAUTH2_NAME: ${PGADMIN_OAUTH2_NAME:-Keycloak}
      PGADMIN_OAUTH2_DISPLAY_NAME: ${PGADMIN_OAUTH2_DISPLAY_NAME:-Login with Keycloak}
      PGADMIN_OAUTH2_CLIENT_ID: ${PGADMIN_OAUTH2_CLIENT_ID:-pgadmin-client}
      PGADMIN_OAUTH2_CLIENT_SECRET: ${PGADMIN_OAUTH2_CLIENT_SECRET:-}
      PGADMIN_OAUTH2_SCOPE: ${PGADMIN_OAUTH2_SCOPE:-openid email profile}
      KEYCLOAK_URL: ${KEYCLOAK_URL:-http://keycloak:8080}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM:-infra-admin}
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./docker/pgadmin/servers.json:/pgadmin4/servers.json:ro
      - ./docker/pgadmin/config_local.py:/pgadmin4/config_local.py:ro
      - ./docker/pgadmin/config_distro.py:/pgadmin4/config_distro.py:ro
    networks:
      - database-net
      - frontend-net
      - monitoring-net
    labels:
      com.docker.compose.service: "pgadmin"
      logging.source: "pgadmin"
      logging.type: "admin_tool"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/pgadmin/login"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ============================================
  # MONITORING: POSTGRESQL EXPORTER
  # ============================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: ai_infra_postgres_exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-app_db}?sslmode=disable"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M

  # ============================================
  # BACKEND WORKERS (AI_Backend)
  # ============================================
  # Built from external AI_Backend repository (../AI_Backend)
  
  # Celery Beat (Scheduler)
  celery_beat:
    build:
      context: ../AI_Backend
      dockerfile: Dockerfile
    container_name: ai_backend_celery_beat
    restart: unless-stopped
    command: celery -A shared.infrastructure.celery_app beat --loglevel=info
    environment:
      # Connect to AI_Infra infrastructure services
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ${BACKEND_DB_NAME:-ai_backend}
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: db+postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${BACKEND_DB_NAME:-ai_backend}
      LOG_LEVEL: INFO
      LOG_FORMAT: json
      ENVIRONMENT: development
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'celery.*beat' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # Email Processor Worker
  email_worker:
    build:
      context: ../AI_Backend
      dockerfile: Dockerfile
    container_name: ai_backend_email_worker
    restart: unless-stopped
    command: celery -A shared.infrastructure.celery_app worker --loglevel=info --queues email_processing --hostname=email_worker@%h --concurrency=4
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ${BACKEND_DB_NAME:-ai_backend}
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: db+postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${BACKEND_DB_NAME:-ai_backend}
      LOG_LEVEL: INFO
      LOG_FORMAT: json
      ENVIRONMENT: development
      PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: 9091
    ports:
      - "9091:9091"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "celery -A shared.infrastructure.celery_app inspect ping -d email_worker@$$HOSTNAME || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Payment Processor Worker
  payment_worker:
    build:
      context: ../AI_Backend
      dockerfile: Dockerfile
    container_name: ai_backend_payment_worker
    restart: unless-stopped
    command: celery -A shared.infrastructure.celery_app worker --loglevel=info --queues payment_processing --hostname=payment_worker@%h --concurrency=4
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ${BACKEND_DB_NAME:-ai_backend}
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: db+postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${BACKEND_DB_NAME:-ai_backend}
      LOG_LEVEL: INFO
      LOG_FORMAT: json
      ENVIRONMENT: development
      PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: 9092
    ports:
      - "9092:9092"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "celery -A shared.infrastructure.celery_app inspect ping -d payment_worker@$$HOSTNAME || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Data Sync Worker
  data_sync_worker:
    build:
      context: ../AI_Backend
      dockerfile: Dockerfile
    container_name: ai_backend_data_sync_worker
    restart: unless-stopped
    command: celery -A shared.infrastructure.celery_app worker --loglevel=info --queues data_sync --hostname=data_sync_worker@%h --concurrency=2
    environment:
      DB_HOST: postgres
      DB_PORT: 5432
      DB_NAME: ${BACKEND_DB_NAME:-ai_backend}
      DB_USER: ${POSTGRES_USER:-postgres}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      REDIS_HOST: redis
      REDIS_PORT: 6379
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: db+postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${BACKEND_DB_NAME:-ai_backend}
      LOG_LEVEL: INFO
      LOG_FORMAT: json
      ENVIRONMENT: development
      PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: 9093
    ports:
      - "9093:9093"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "celery -A shared.infrastructure.celery_app inspect ping -d data_sync_worker@$$HOSTNAME || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # Flower (Celery monitoring UI)
  flower:
    build:
      context: ../AI_Backend
      dockerfile: Dockerfile
    container_name: ai_backend_flower
    restart: unless-stopped
    command: celery -A shared.infrastructure.celery_app flower --broker=redis://redis:6379/0 --port=5555
    environment:
      CELERY_BROKER_URL: redis://redis:6379/0
      CELERY_RESULT_BACKEND: db+postgresql+psycopg://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${BACKEND_DB_NAME:-ai_backend}
    ports:
      - "5555:5555"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --spider -q http://localhost:5555 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
        reservations:
          cpus: '0.05'
          memory: 64M

  # ============================================
  # OBJECT STORAGE: MinIO CLUSTER
  # ============================================
  # Distributed 4-node MinIO cluster for S3-compatible object storage
  
  minio1:
    image: minio/minio:latest
    container_name: ai_infra_minio1
    restart: unless-stopped
    command: server --console-address ":9001" http://minio{1...4}/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
      MINIO_DOMAIN: ${MINIO_DOMAIN:-storage.intra.local}
      MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-http://localhost/minio-console}
      # Keycloak OIDC Integration
      MINIO_IDENTITY_OPENID_CONFIG_URL: ${MINIO_IDENTITY_OPENID_CONFIG_URL:-http://keycloak:8080/auth/realms/infra-admin/.well-known/openid-configuration}
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${MINIO_IDENTITY_OPENID_CLIENT_ID:-minio-console}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${MINIO_IDENTITY_OPENID_CLIENT_SECRET}
      MINIO_IDENTITY_OPENID_CLAIM_NAME: ${MINIO_IDENTITY_OPENID_CLAIM_NAME:-policy}
      MINIO_IDENTITY_OPENID_SCOPES: ${MINIO_IDENTITY_OPENID_SCOPES:-openid,profile,email}
      MINIO_IDENTITY_OPENID_REDIRECT_URI: ${MINIO_IDENTITY_OPENID_REDIRECT_URI:-http://localhost/minio-console/oauth_callback}
      # Prometheus metrics
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus:9090
      MINIO_PROMETHEUS_JOB_ID: minio-cluster
      # Console settings
      MINIO_BROWSER: on
      MINIO_REGION: ${MINIO_REGION:-us-east-1}
    volumes:
      - minio1_data:/data
    networks:
      - storage-net
      - monitoring-net
      - database-net
    labels:
      com.docker.compose.service: "minio1"
      logging.source: "minio"
      logging.type: "object_storage"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  minio2:
    image: minio/minio:latest
    container_name: ai_infra_minio2
    restart: unless-stopped
    command: server --console-address ":9001" http://minio{1...4}/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
      MINIO_DOMAIN: ${MINIO_DOMAIN:-storage.intra.local}
      MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-http://localhost/minio-console}
      # Keycloak OIDC Integration
      MINIO_IDENTITY_OPENID_CONFIG_URL: ${MINIO_IDENTITY_OPENID_CONFIG_URL:-http://keycloak:8080/auth/realms/infra-admin/.well-known/openid-configuration}
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${MINIO_IDENTITY_OPENID_CLIENT_ID:-minio-console}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${MINIO_IDENTITY_OPENID_CLIENT_SECRET}
      MINIO_IDENTITY_OPENID_CLAIM_NAME: ${MINIO_IDENTITY_OPENID_CLAIM_NAME:-policy}
      MINIO_IDENTITY_OPENID_SCOPES: ${MINIO_IDENTITY_OPENID_SCOPES:-openid,profile,email}
      MINIO_IDENTITY_OPENID_REDIRECT_URI: ${MINIO_IDENTITY_OPENID_REDIRECT_URI:-http://localhost/minio-console/oauth_callback}
      # Prometheus metrics
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus:9090
      MINIO_PROMETHEUS_JOB_ID: minio-cluster
      # Console settings
      MINIO_BROWSER: on
      MINIO_REGION: ${MINIO_REGION:-us-east-1}
    volumes:
      - minio2_data:/data
    networks:
      - storage-net
      - monitoring-net
      - database-net
    labels:
      com.docker.compose.service: "minio2"
      logging.source: "minio"
      logging.type: "object_storage"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  minio3:
    image: minio/minio:latest
    container_name: ai_infra_minio3
    restart: unless-stopped
    command: server --console-address ":9001" http://minio{1...4}/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
      MINIO_DOMAIN: ${MINIO_DOMAIN:-storage.intra.local}
      MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-http://localhost/minio-console}
      # Keycloak OIDC Integration
      MINIO_IDENTITY_OPENID_CONFIG_URL: ${MINIO_IDENTITY_OPENID_CONFIG_URL:-http://keycloak:8080/auth/realms/infra-admin/.well-known/openid-configuration}
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${MINIO_IDENTITY_OPENID_CLIENT_ID:-minio-console}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${MINIO_IDENTITY_OPENID_CLIENT_SECRET}
      MINIO_IDENTITY_OPENID_CLAIM_NAME: ${MINIO_IDENTITY_OPENID_CLAIM_NAME:-policy}
      MINIO_IDENTITY_OPENID_SCOPES: ${MINIO_IDENTITY_OPENID_SCOPES:-openid,profile,email}
      MINIO_IDENTITY_OPENID_REDIRECT_URI: ${MINIO_IDENTITY_OPENID_REDIRECT_URI:-http://localhost/minio-console/oauth_callback}
      # Prometheus metrics
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus:9090
      MINIO_PROMETHEUS_JOB_ID: minio-cluster
      # Console settings
      MINIO_BROWSER: on
      MINIO_REGION: ${MINIO_REGION:-us-east-1}
    volumes:
      - minio3_data:/data
    networks:
      - storage-net
      - monitoring-net
      - database-net
    labels:
      com.docker.compose.service: "minio3"
      logging.source: "minio"
      logging.type: "object_storage"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

  minio4:
    image: minio/minio:latest
    container_name: ai_infra_minio4
    restart: unless-stopped
    command: server --console-address ":9001" http://minio{1...4}/data
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
      MINIO_DOMAIN: ${MINIO_DOMAIN:-storage.intra.local}
      MINIO_BROWSER_REDIRECT_URL: ${MINIO_BROWSER_REDIRECT_URL:-http://localhost/minio-console}
      # Keycloak OIDC Integration
      MINIO_IDENTITY_OPENID_CONFIG_URL: ${MINIO_IDENTITY_OPENID_CONFIG_URL:-http://keycloak:8080/auth/realms/infra-admin/.well-known/openid-configuration}
      MINIO_IDENTITY_OPENID_CLIENT_ID: ${MINIO_IDENTITY_OPENID_CLIENT_ID:-minio-console}
      MINIO_IDENTITY_OPENID_CLIENT_SECRET: ${MINIO_IDENTITY_OPENID_CLIENT_SECRET}
      MINIO_IDENTITY_OPENID_CLAIM_NAME: ${MINIO_IDENTITY_OPENID_CLAIM_NAME:-policy}
      MINIO_IDENTITY_OPENID_SCOPES: ${MINIO_IDENTITY_OPENID_SCOPES:-openid,profile,email}
      MINIO_IDENTITY_OPENID_REDIRECT_URI: ${MINIO_IDENTITY_OPENID_REDIRECT_URI:-http://localhost/minio-console/oauth_callback}
      # Prometheus metrics
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_PROMETHEUS_URL: http://prometheus:9090
      MINIO_PROMETHEUS_JOB_ID: minio-cluster
      # Console settings
      MINIO_BROWSER: on
      MINIO_REGION: ${MINIO_REGION:-us-east-1}
    volumes:
      - minio4_data:/data
    networks:
      - storage-net
      - monitoring-net
      - database-net
    labels:
      com.docker.compose.service: "minio4"
      logging.source: "minio"
      logging.type: "object_storage"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M

# ============================================
# NETWORKS
# ============================================
networks:
  frontend-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/24
  monitoring-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/24
  database-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24
  storage-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.40.0.0/24

# ============================================
# VOLUMES
# ============================================
volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  tempo_data:
    driver: local
  loki_data:
    driver: local
  postgres_data:
    driver: local
  pgadmin_data:
    driver: local
  keycloak_data:
    driver: local
  redis_data:
    driver: local
  minio1_data:
    driver: local
  minio2_data:
    driver: local
  minio3_data:
    driver: local
  minio4_data:
    driver: local
