services:
  # ============================================
  # FRONTEND APPLICATION
  # ============================================
  frontend:
    build:
      context: ./frontend/ai-front
      dockerfile: Dockerfile
    container_name: ai_infra_frontend
    restart: unless-stopped
    networks:
      - frontend-net
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # REVERSE PROXY
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: ai_infra_nginx
    restart: unless-stopped
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    ports:
      - "${NGINX_PORT:-80}:80"
    networks:
      - frontend-net
      - monitoring-net
    # Removed hard dependencies to allow Nginx to start even if services are not ready
    # Runtime DNS resolution in nginx.conf handles service discovery dynamically
    depends_on:
      - frontend
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # MONITORING: PROMETHEUS
  # ============================================
  prometheus:
    image: prom/prometheus:latest
    container_name: ai_infra_prometheus
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.external-url=/monitoring/prometheus'
      - '--web.route-prefix=/'
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./docker/prometheus/alerts:/etc/prometheus/alerts
      - prometheus_data:/prometheus
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: GRAFANA
  # ============================================
  grafana:
    image: grafana/grafana:latest
    container_name: ai_infra_grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: ${GRAFANA_PLUGINS:-}
      GF_USERS_ALLOW_SIGN_UP: false
      # Subpath configuration for reverse proxy
      # NOTE: These environment variables override grafana.ini settings
      # serve_from_sub_path=false: NGINX passes FULL path to Grafana
      # Grafana handles the subpath (/monitoring/grafana/) internally
      GF_SERVER_ROOT_URL: http://localhost/monitoring/grafana/
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_SERVER_DOMAIN: localhost
      # Config file path
      GF_PATHS_CONFIG: /etc/grafana/grafana.ini
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards
      - ./docker/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
    networks:
      - monitoring-net
    depends_on:
      - prometheus
      - tempo
      - loki
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 128M

  # ============================================
  # MONITORING: TEMPO (Distributed Tracing)
  # ============================================
  tempo:
    image: grafana/tempo:latest
    container_name: ai_infra_tempo
    restart: unless-stopped
    command: ["-config.file=/etc/tempo/tempo.yml"]
    volumes:
      - ./docker/tempo/tempo.yml:/etc/tempo/tempo.yml
      - tempo_data:/var/tempo
    # No ports exposed - all access via nginx at /monitoring/tempo/
    # OTLP receivers (4317, 4318) available internally within monitoring-net
    networks:
      - monitoring-net
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: LOKI (Log Aggregation)
  # ============================================
  loki:
    image: grafana/loki:latest
    container_name: ai_infra_loki
    restart: unless-stopped
    command: ["-config.file=/etc/loki/loki.yml"]
    volumes:
      - ./docker/loki/loki.yml:/etc/loki/loki.yml
      - loki_data:/loki
    # HTTP UI accessible via nginx at /monitoring/loki/
    # Port 3100 used internally for log ingestion from Promtail/clients
    networks:
      - monitoring-net
    # Note: Loki container doesn't include health check tools
    # Health can be monitored via Prometheus or Grafana
    # healthcheck:
    #   test: ["CMD", "wget", "--spider", "-q", "http://localhost:3100/ready"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # MONITORING: PROMTAIL (Log Collection Agent)
  # ============================================
  promtail:
    image: grafana/promtail:latest
    container_name: ai_infra_promtail
    restart: unless-stopped
    command: ["-config.file=/etc/promtail/promtail.yml"]
    volumes:
      - ./docker/promtail/promtail.yml:/etc/promtail/promtail.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - monitoring-net
    depends_on:
      - loki
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9080/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M

  # ============================================
  # DATABASE: POSTGRESQL
  # ============================================
  postgres:
    image: postgres:16-alpine
    container_name: ai_infra_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-app_db}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-postgres}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=en_US.UTF-8"
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_LOG_LEVEL: ${POSTGRES_LOG_LEVEL:-WARNING}
      POSTGRES_SLOW_QUERY_MS: ${POSTGRES_SLOW_QUERY_MS:-1000}
      # Keycloak database credentials
      KEYCLOAK_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      - ./docker/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf:ro
      - ./docker/postgres/init:/docker-entrypoint-initdb.d:ro
    command: postgres -c config_file=/etc/postgresql/postgresql.conf -c hba_file=/etc/postgresql/pg_hba.conf
    networks:
      - database-net
      - monitoring-net
    labels:
      com.docker.compose.service: "postgres"
      logging.source: "postgres"
      logging.type: "database"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-app_db}"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================
  # IDENTITY & ACCESS MANAGEMENT: KEYCLOAK
  # ============================================
  keycloak:
    image: quay.io/keycloak/keycloak:latest
    container_name: ai_infra_keycloak
    restart: unless-stopped
    command: start-dev --import-realm
    environment:
      KC_DB: postgres
      KC_DB_URL_HOST: postgres
      KC_DB_URL_DATABASE: keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:-keycloak}
      KC_DB_SCHEMA: public
      KEYCLOAK_ADMIN: ${KEYCLOAK_ADMIN:-admin}
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:-admin}
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HOSTNAME_STRICT_BACKCHANNEL: false
      KC_HTTP_ENABLED: true
      KC_PROXY: edge
      KC_PROXY_ADDRESS_FORWARDING: true
      KC_HEALTH_ENABLED: true
      KC_METRICS_ENABLED: true
      KC_LOG_LEVEL: ${KEYCLOAK_LOG_LEVEL:-INFO}
    volumes:
      - keycloak_data:/opt/keycloak/data
      - ./docker/keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json:ro
      - ./docker/keycloak/keycloak.conf:/opt/keycloak/conf/keycloak.conf:ro
    networks:
      - frontend-net
      - database-net
      - monitoring-net
    labels:
      com.docker.compose.service: "keycloak"
      logging.source: "keycloak"
      logging.type: "identity_provider"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # ============================================
  # DATABASE ADMIN: PGADMIN
  # ============================================
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: ai_infra_pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-admin@example.com}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-admin}
      PGADMIN_CONFIG_SERVER_MODE: "True"
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: "False"
      PGADMIN_LOG_LEVEL: ${PGADMIN_LOG_LEVEL:-INFO}
      SCRIPT_NAME: /pgadmin
      ENVIRONMENT: ${ENVIRONMENT:-development}
      # Gunicorn timeout configuration (in seconds) - prevent startup errors
      GUNICORN_TIMEOUT: ${PGADMIN_GUNICORN_TIMEOUT:-60}
      # Keycloak OIDC Configuration
      # Note: AUTHENTICATION_SOURCES is configured in config_distro.py and config_local.py
      PGADMIN_CONFIG_OAUTH2_AUTO_CREATE_USER: ${PGADMIN_OAUTH2_AUTO_CREATE_USER:-True}
      PGADMIN_OAUTH2_NAME: ${PGADMIN_OAUTH2_NAME:-Keycloak}
      PGADMIN_OAUTH2_DISPLAY_NAME: ${PGADMIN_OAUTH2_DISPLAY_NAME:-Login with Keycloak}
      PGADMIN_OAUTH2_CLIENT_ID: ${PGADMIN_OAUTH2_CLIENT_ID:-pgadmin-client}
      PGADMIN_OAUTH2_CLIENT_SECRET: ${PGADMIN_OAUTH2_CLIENT_SECRET:-}
      PGADMIN_OAUTH2_SCOPE: ${PGADMIN_OAUTH2_SCOPE:-openid email profile}
      KEYCLOAK_URL: ${KEYCLOAK_URL:-http://keycloak:8080}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM:-infra-admin}
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - ./docker/pgadmin/servers.json:/pgadmin4/servers.json:ro
      - ./docker/pgadmin/config_local.py:/pgadmin4/config_local.py:ro
      - ./docker/pgadmin/config_distro.py:/pgadmin4/config_distro.py:ro
    networks:
      - database-net
      - frontend-net
      - monitoring-net
    labels:
      com.docker.compose.service: "pgadmin"
      logging.source: "pgadmin"
      logging.type: "admin_tool"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "com.docker.compose.service,logging.source,logging.type"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:80/pgadmin/login"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ============================================
  # MONITORING: POSTGRESQL EXPORTER
  # ============================================
  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: ai_infra_postgres_exporter
    restart: unless-stopped
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER:-postgres}:${POSTGRES_PASSWORD:-postgres}@postgres:5432/${POSTGRES_DB:-app_db}?sslmode=disable"
    networks:
      - database-net
      - monitoring-net
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9187/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 32M

# ============================================
# NETWORKS
# ============================================
networks:
  frontend-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.50.0.0/24
  monitoring-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/24
  database-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.23.0.0/24

# ============================================
# VOLUMES
# ============================================
volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  tempo_data:
    driver: local
  loki_data:
    driver: local
  postgres_data:
    driver: local
  pgadmin_data:
    driver: local
  keycloak_data:
    driver: local
